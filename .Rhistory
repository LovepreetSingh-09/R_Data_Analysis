y = col_character() ))
tail(challenge)
challenge <- read_csv(
readr_example("challenge.csv"),
col_types = cols(
x = col_double(),
y = col_date() ))
tail(challenge)
# reading more than maximum limit of 1000 values of a row
challenge2 <- read_csv(
readr_example("challenge.csv"),
guess_max = 1001
)
challenge2
# reading variables as character
challenge2 <- read_csv(readr_example("challenge.csv"),
col_types = cols(.default = col_character()) )
challenge2
df <- tribble(
~x, ~y,
"1", "1.21",
"2", "2.32",
"3", "4.56"
)
df
# conveting the suitable datatype of each variable
type_convert(df)
write_csv(challenge, "challenge-2.csv")
read_csv("challenge-2.csv")
# rds store data in R’s custom binary format called RDS
write_rds(challenge, "challenge.rds")
read_rds("challenge.rds")
# The feather package implements a fast binary file format that
# can be shared across programming languages
library(feather)
write_feather(challenge, "challenge.feather")
read_feather("challenge.feather")
table1
table2
table3
table4a
table4b
table1
table1 %>%
mutate(rate = cases / population * 10000)
table1 %>%
count(year, wt = cases)
library(ggplot2)
ggplot(table1, aes(year, cases)) +
geom_line(aes(group = country), color = "grey50") +
geom_point(aes(color = country))
# gathering shortens the data table width
table4a
gather(table4a,'1999','2000',key='year',value='cases')
table4b %>%
gather(`1999`, `2000`, key = "year", value = "population")
table4b
tidy4a <- table4a %>%
gather(`1999`, `2000`, key = "year", value = "cases")
tidy4b <- table4b %>%
gather(`1999`, `2000`, key = "year", value = "population")
left_join(tidy4a, tidy4b)
# Spreading increases the data width
table2
spread(table2, key=type, value=count)
# separate() pulls apart one column into multiple columns, by splitting
# wherever a separator character appears.
table3
separate(table3,rate,into=c('cases','population'))
# We can aslo use a separate argument
table3 %>%
separate(rate, into = c("cases", "population"), sep = "/")
# separate make the columns as char type by default
# But we can change it to a better type using convvert=TRUE
table3 %>%
separate(
rate,
into = c("cases", "population"),
convert = TRUE
)
# We can also separate a column using the position number
table3 %>%
separate(year, into = c("century", "year"), sep = 2)
# Unite
table5
unite(table5,new,century, year)
# unite unite the values of two columns using default _ sign
table5 %>%
unite(new, century, year, sep = "")
stocks <- tibble(
year = c(2015, 2015, 2015, 2015, 2016, 2016, 2016),
qtr = c( 1, 2, 3, 4, 2, 3, 4),
return = c(1.88, 0.59, 0.35, NA, 0.92, 0.17, 2.66)
)
spread(year,value=return)
spread(stocks,year,value=return)
spread(stocks,key=year,value=return)
gather(b,2015,2016,key=year,value=rate,na.rm=T)
b=spread(stocks,key=year,value=return)
gather(b,2015,2016,key=year,value=rate,na.rm=T)
gather(b,2015,2016,key=year,value=return,na.rm=T)
gather(b,2015,2016,key=year,value=return,na.rm=TRUE)
gather(b,key=year,value=return,2015,2016,na.rm=TRUE)
gather(b,year,return,2015,2016,na.rm=TRUE)
gather(b,year,return, `2015` , `2016` ,na.rm=TRUE)
gather(b,key=year,value=return, `2015` , `2016` ,na.rm=TRUE)
spread(stocks,key=year,value=return)
complete(stocks,year,return)
treatment <- tribble(
~ person, ~ treatment, ~response,
"Derrick Whitmore", 1, 7,
NA, 2, 10,
NA, 3, 9,
"Katherine Burke", 1, 4
)
fill(treatment,person)
treatment
fill(treatment,person)
who
gather(who,new_sp_m014:newrel_f65,key='key',value='count',na.rm=TRUE)
who1=gather(who,new_sp_m014:newrel_f65,key='key',value='count',na.rm=TRUE)
who1[-iso2,-iso3]
who1[c(-iso2,-iso3)]
who1[c(-'iso2',-'iso3')]
View(who)
count(who1,key)
who2=mutate(key=stringr::str_replace('newrel','new_rel'))
who2=mutate(key=stringr::str_replace(key,'newrel','new_rel'))
who2=mutate(who1,key=stringr::str_replace(key,'newrel','new_rel'))
who2
separate(who2,key,into=c('class','category','age'),sep='_')
who2=separate(who2,key,into=c('class','category','age'),sep='_')
who2 %>% count(class)
who2=select(-iso2,-iso3)
who2=select(who2,-class,-iso2,-iso3)
who2
separate(who2,age,into=c('gender','age'),sep=1)
who_final=separate(who2,age,into=c('gender','age'),sep=1)
who_final
library(tidyverse)
library(nycflights13)
airlines
planes
flights
weather
flights %>% count(tailnum) %>% filter(n>1)
weather %>% count(year,month,day,origin) %>% filter(n>1)
weather %>% count(year,month,day,hour,origin) %>% filter(n>1)
flights %>% count(year,month,day,tailnum) %>% filter(n>1)
flights2 <- flights %>%
select(year:day, hour, origin, dest, tailnum, carrier)
flights2
airlines
flights2 %>%
select(-origin, -dest) %>%
left_join(airlines, by = "carrier")
count(airlines,carrier)
flights2 %>%
select(-origin, -dest) %>%
left_join(airlines, by = "carrier")
flights2 %>%
select(-origin, -dest) %>%
mutate(name=airlines$name[match(carrier,airlines$carrier)])
# Duplicate Joins
x <- tribble(
~key, ~val_x,
1, "x1",
2, "x2",
2, "x3",
1, "x4"
)
y <- tribble(
~key, ~val_y,
1, "y1",
2, "y2"
)
left_join(x, y, by = "key")
x <- tribble(
~key, ~val_x,
1, "x1",
2, "x2",
2, "x3",
3, "x4"
)
y <- tribble(
~key, ~val_y,
1, "y1",
2, "y2",
2, "y3",
3, "y4"
)
left_join(x, y, by = "key")
left_join(flights2,weather)
weather
flights2
count(weather,origin)
count(flights2,origin)
left_join(flights2,weather)
weather
weather
flights2
count(weather,origin)
count(flights2,origin)
left_join(flights2,weather)
planes
count(planes,tailnum)
planes
flights2 %>% left_join(planes,by='tailnum')
flights2
airportd
airports
count(airports,faa)
#
count(airports,name)
flights2
airports
flights2 %>% left_join(airline,by=c('dest'='faa'))
flights2 %>% left_join(airports,by=c('dest'='faa'))
count(flights2,dest)
# faa and name which is identity of an airport is just once and hence it is a unique key
count(airports,name)
# faa and name which is identity of an airport is just once and hence it is a unique key
count(airports,faa)
count(flights2,dest)
airports
flights2
flights2 %>% left_join(airports,by=c('dest'='faa'))
view(airports)
flights2 %>% left_join(airports,by=c('origin'='faa'))
airports %>%
semi_join(flights, c("faa" = "dest")) %>%
ggplot(aes(lon, lat)) +
borders("state") +geom_point() +
coord_quickmap()
top_dest <- flights %>%
count(dest, sort = TRUE) %>%
head(10)
top_dest
flights %>% filter(dest %in% top_dest$dest)
# This can be easily done by semi_join which represent the intersect or common values
semi_join(flights,top_dest)
# anti_join represent the values ehich are only in the left dataset not in the 2nd one.
anti_join(flights,planes,by='tailnum') %>%
count(tailnum,sort=T)
# set operations :-
# intersect for common values
# union for both the values
# setdiff for values in x not in y
df1 <- tribble(
~x, ~y,
1, 1,
2, 1
)
df2 <- tribble(
~x, ~y,
1, 1,
1, 2
)
intersect(df1, df2)
union(df1,df2)
setdiff(df1,df2)
setdiff(df2.df1)
setdiff(df2,df1)
library(tidyverse)
library(nycflights13)
str_length(c("a", "R for data science", NA))
# combine strings
str_c(c("x","y"))
# combine strings
str_c("x","y")
str_c("x","y","z",sep=',')
str_c('|-',x,'-|')
x=c("abc",NA)
str_c('|-',x,'-|')
str_c("|-",str_replace_na(x),"-|")
str_c("prefix-", c("a", "b", "c"), "-suffix")
str_c(c("x", "y", "z"), collapse = ", ")
str_sub(x,1,4)
# sunbsetting strings
x <- c("Apple", "Banana", "Pear")
str_sub(x,1,4)
str_sub(x,-3,-1)
str_sub(x,1,1)=str_to_lower(str_sub(x,1,1))
x
# Locales
str_to_upper(c("i", "ı"))
str_to_upper(c("i", "ı"),locale='tr')
str_sort(x,locale='hi')
str_sort(x,locale='pun')
str_sort(x,locale='haw')
str_sort(x, locale = "en")
str_sort(x,locale='haw')
x <- c("apple", "eggplant", "banana")
str_sort(x, locale = "en")
str_sort(x,locale='haw')
str_sort(x,locale='pun')
source('~/R/R_Data_Analysis/stringr.R', encoding = 'UTF-8')
str_view(x,'a,')
str_view(x,'a.')
str_view(x,'.a.')
install.packages("htmlwidgets")
install.packages("htmlwidgets")
library(htmlwidgets)
str_view(x,'.a.')
# To create the regular expression, we need \\
dot <- "\\."
# But the expression itself only contains one:
writeLines(dot)
# And this tells R to look for an explicit .
str_view(c("abc", "a.c", "bef"), "a\\.c")
x='a\\b'
str_view(x,'\\\\')
x <- c("apple", "banana", "pear")
str_view(x, "^a")
str_view(x,'a$')
str_view(x, "apple4")
str_view(x, "apple$")
x <- c("apple pie", "apple", "apple cake")
str_view(x, "apple$")
str_view(c("grey", "gray"),'gr(e|a)y')
x <- "1888 is the longest year in Roman numerals: MDCCCLXXXVIII"
str_view(x, "CC?")
str_view(x,'CC+')
str_view(x,'C[LX]+')
str_view(x,'C{2}')
str_view(x,'C{3}')
str_view(x,'C{2,3}')
# RegExp shows greedy behaviour means takes the longest streak of match
# To find the short match we can use ? after the expression
str_view(x,'C[LX]+?')
str_view(fruit, "(..)\\1", match = TRUE)
str_view(fruit, "(..)\\2", match = TRUE)
#
fruits
#
fruit
x<- c("apple", "banana", "pear")
str_detect(x, "e")
str_match(fruit,'(..)\\1')
str_match_all(fruit,'(..)\\1')
str_match(fruit,'(..)\\1')
sum(str_detect(words,'^t'))
words
mean(str_detect(words,'[aeiou]$'))
no_vowels_1 <- !str_detect(words, "[aeiou]")
no_vowels_2 <- str_detect(words,'^[^aeiou]$')
identical(n0_vowels1==no_vowels_2)
identical(no_vowels1==no_vowels_2)
no_vowels_1 <- !str_detect(words, "[aeiou]")
no_vowels_2 <- str_detect(words,'^[^aeiou]$')
identical(no_vowels1==no_vowels_2)
identical(no_vowels_1==no_vowels_2)
identical(no_vowels_1,no_vowels_2)
no_vowels_1 <- !str_detect(words, "[aeiou]")
no_vowels_2 <- str_detect(words,'^[^aeiou]$')
identical(no_vowels_1,no_vowels_2)
no_vowels_2 <- str_detect(words,'^[^aeiou]+$')
identical(no_vowels_1,no_vowels_2)
words[str_detect(words,'x$')]
# This is same as
str_sub(words,'x$')
# This is same as
str_subset(words,'x$')
df <- tibble(
word = words,
i = seq_along(word))
df
df %>% str_detect(word,'x$')
df %>% filter(str_detect(word,'x$'))
str_count(x,'a')
mean(str_count(words, "[aeiou]"))
df %>% mutate(vowels=str_count(word,'[aeiou]'),
consonants=str_count(word,'[^aeiou]'))
str_count("abababa", "aba")
str_view('abababa','aba')
str_view_all('abababa','aba')
length(sentences)
head(sentences)
colors <- c(
"red", "orange", "yellow", "green", "blue", "purple")
color_match <- str_c(colors, collapse = "|")
color_match
str_subset(sentences,colour_match)
str_subset(sentences,color_match)
subset <- str_subset(sentences,color_match)
str_extract(subset,color_match)
more <- sentences[str_count(sentences, color_match) > 1]
more
str_count(sentences, color_match) > 1
str_count("abababa", "aba")
str_count(sentences, color_match)
more
str_extract(more,color_match)
str_extract_all(more,color_match)
str_extract_all(more,color_match,simplify = T)
str_extract_all(more,color_match,simplify = TRUE)
x <- c("a", "a b", "a b c")
str_extract_all(x, "[a-z]", simplify = TRUE)
noun='(the|a) ([^ ]+)'
has_noun <- sentences %>%
str_subset(noun) %>%
head(10)
has_noun
has_noun %>% str_extract_all(noun)
has_noun %>% str_extract(noun)
has_noun %>% str_extract_all(noun,simplify=TRUE)
has_noun %>% str_extract(noun,simplify=TRUE)
has_noun %>% str_extract(noun)
has_noun %>% str_match(noun)
tibble(sentence=sentences) %>%
tidyr::extract(sentence,c('article','noun'),'noun')
tibble(sentence=sentences) %>%
tidyr::extract(sentence,c('article','noun'),noun)
tibble(sentence=sentences) %>%
tidyr::extract(sentence,c('article','noun'),noun,remove=FALSE)
str_replace(x,'[aeiou]','-')
str_replace_all(x,'[aeiou]','-')
str_replace(x,'[aeiou]','-')
str_replace(x, "[aeiou]", "-")
x <- c("apple", "pear", "banana")
str_replace(x,'[aeiou]','-')
str_replace_all(x,'[aeiou]','-')
x <- c("1 house", "2 cars", "3 people")
str_replace_all(x,c('1'='one','2'='two','3'='three'))
sentences %>% head(5)
sentences %>%
str_replace("([^ ]+) ([^ ]+) ([^ ]+)", "\\1 \\3 \\2") %>%
head(5)
# flipping the 3rd and 2nd word of string
sentences %>%
str_replace("([^ ]+) ([^ ]+) ([^ ]+)", "\\1 \\3 \\2") %>%
head(5)
sentences %>%
head(5) %>%
str_split(" ")
"a|b|c|d" %>%
str_split('\\|')
"a|b|c|d" %>%
str_split('\\|') %>% .[1]
"a|b|c|d" %>%
str_split('\\|') %>% .[[1]]
"a|b|c|d" %>%
str_split('\\|') %>% [[1]] # .
sentences %>%
head(5) %>%
str_split(" ", simplify = TRUE)
fields %>% str_split(": ", n = 3, simplify = TRUE)
fields %>% str_split(": ", n = 2, simplify = TRUE)
fields <- c("Name: Hadley", "Country: NZ", "Age: 35")
fields %>% str_split(": ", n = 3, simplify = TRUE)
fields %>% str_split(": ", n = 1, simplify = TRUE)
fields %>% str_split(": ", n = 4, simplify = TRUE)
fields %>% str_split(": ", n = 2, simplify = TRUE)
x <- "This is a sentence. This is another sentence."
str_view_all(x, boundary("word"))
str_view(x, boundary("word"))
str_split(x, boundary("word"))[[1]]
str_view(fruit, regex("nana"))
# multiline = TRUE allows ^ and $ to match the start and end of
# each line rather than the start and end of the complete string
x <- "Line 1\nLine 2\nLine 3"
str_extract_all(x, "^Line")[[1]]
str_extract_all(x, "^Line")
str_extract_all(x, regex("^Line", multiline = TRUE))[[1]]
phone <- regex("
\\(? # optional opening parens
(\\d{3}) # area code
[)- ]? # optional closing parens, dash, or space
(\\d{3}) # another three numbers
[ -]? # optional space or dash
(\\d{3}) # three more numbers
", comments = TRUE)
str_match("514-791-8141", phone)
# fixed() matches exactly the specified sequence of bytes. It ignores all special regular expressions
# and operates at a very low level. This allows you to avoid complex escaping and can be
# much faster than regular expressions.
microbenchmark::microbenchmark(
fixed = str_detect(sentences, fixed("the")),
regex = str_detect(sentences, "the"),
times = 20)
install.packages("microbenchmark")
a1 <- "\u00e1"
a2 <- "a\u0301"
library(microbenchmark)
# fixed() matches exactly the specified sequence of bytes. It ignores all special regular expressions
# and operates at a very low level. This allows you to avoid complex escaping and can be
# much faster than regular expressions.
microbenchmark::microbenchmark(
fixed = str_detect(sentences, fixed("the")),
regex = str_detect(sentences, "the"),
times = 20)
a1 <- "\u00e1"
a2 <- "a\u0301"
c(a1, a2)
a1 == a2
str_detect(a1, fixed(a2))
str_detect(a1, coll(a2)) # TRUE
i <- c("I", "İ", "i", "ı")
i
str_subset(i, coll("i", ignore_case = TRUE))
str_subset(
i,
coll("i", ignore_case = TRUE, locale = "tr") )
str_subset( i, coll("i", ignore_case = TRUE, locale = "tr") )
stringi::stri_locale_info()
# apropos() searches all objects available from the global environment.
apropos("replace")
# apropos() searches all objects available from the global environment.
apropos("count")
head(dir(pattern = "\\.Rmd$"))
# dir() lists all the files in a directory.
head(dir(pattern = "\\.Rmd$"))
